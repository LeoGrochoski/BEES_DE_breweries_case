{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BEES - Breweries Data Pipeline Bem-vindo \u00e0 documenta\u00e7\u00e3o do Breweries Data Pipeline. Este projeto \u00e9 respons\u00e1vel por coletar, transformar e agregar dados de cervejarias usando a API OpenBreweryDB como forma de demonstrar minhas capacidades na constru\u00e7\u00e3o de pipeline de dados. Estrutura do Projeto O pipeline \u00e9 composto por tr\u00eas etapas principais, cada uma com seu pr\u00f3prio script Python e DAG do Airflow correspondente: Busca de Dados Transforma\u00e7\u00e3o de Dados Agrega\u00e7\u00e3o de Dados Scripts Busca de Dados Transforma\u00e7\u00e3o de Dados Agrega\u00e7\u00e3o de Dados DAGs DAG de orquestra\u00e7\u00e3o do script Busca de Dados DAG de Transforma\u00e7\u00e3o de Dados DAG de Agrega\u00e7\u00e3o de Dados","title":"Home"},{"location":"#bees-breweries-data-pipeline","text":"Bem-vindo \u00e0 documenta\u00e7\u00e3o do Breweries Data Pipeline. Este projeto \u00e9 respons\u00e1vel por coletar, transformar e agregar dados de cervejarias usando a API OpenBreweryDB como forma de demonstrar minhas capacidades na constru\u00e7\u00e3o de pipeline de dados.","title":"BEES - Breweries Data Pipeline"},{"location":"#estrutura-do-projeto","text":"O pipeline \u00e9 composto por tr\u00eas etapas principais, cada uma com seu pr\u00f3prio script Python e DAG do Airflow correspondente: Busca de Dados Transforma\u00e7\u00e3o de Dados Agrega\u00e7\u00e3o de Dados","title":"Estrutura do Projeto"},{"location":"#scripts","text":"Busca de Dados Transforma\u00e7\u00e3o de Dados Agrega\u00e7\u00e3o de Dados","title":"Scripts"},{"location":"#dags","text":"DAG de orquestra\u00e7\u00e3o do script Busca de Dados DAG de Transforma\u00e7\u00e3o de Dados DAG de Agrega\u00e7\u00e3o de Dados","title":"DAGs"},{"location":"dags/agregando_dados_dag/","text":"agregando_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de agrega\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script agregando_dados.py . Principais Componentes execute_script() Executa o script agregando_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a agrega\u00e7\u00e3o de dados. default_args Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha. dag Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG. agregando_dados_task Tarefa que executa o script de agrega\u00e7\u00e3o de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence. C\u00f3digo Completo ```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime import subprocess \"\"\" agregando_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de agrega\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'agregando_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - agregando_dados_task: Tarefa que executa o script de agrega\u00e7\u00e3o. \"\"\" def execute_script(): \"\"\" Executa o script 'agregando_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a agrega\u00e7\u00e3o de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/agregando_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'agregando_dados_dag', default_args=default_args, schedule_interval='@daily', ) agregando_dados_task = PythonOperator( task_id='agregando_dados_task', python_callable=execute_script, dag=dag, )","title":"Agrega\u00e7\u00e3o de Dados DAG"},{"location":"dags/agregando_dados_dag/#agregando_dados_dagpy","text":"Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de agrega\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script agregando_dados.py .","title":"agregando_dados_dag.py"},{"location":"dags/agregando_dados_dag/#principais-componentes","text":"","title":"Principais Componentes"},{"location":"dags/agregando_dados_dag/#execute_script","text":"Executa o script agregando_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a agrega\u00e7\u00e3o de dados.","title":"execute_script()"},{"location":"dags/agregando_dados_dag/#default_args","text":"Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha.","title":"default_args"},{"location":"dags/agregando_dados_dag/#dag","text":"Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG.","title":"dag"},{"location":"dags/agregando_dados_dag/#agregando_dados_task","text":"Tarefa que executa o script de agrega\u00e7\u00e3o de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence.","title":"agregando_dados_task"},{"location":"dags/agregando_dados_dag/#codigo-completo","text":"```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime import subprocess \"\"\" agregando_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de agrega\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'agregando_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - agregando_dados_task: Tarefa que executa o script de agrega\u00e7\u00e3o. \"\"\" def execute_script(): \"\"\" Executa o script 'agregando_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a agrega\u00e7\u00e3o de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/agregando_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'agregando_dados_dag', default_args=default_args, schedule_interval='@daily', ) agregando_dados_task = PythonOperator( task_id='agregando_dados_task', python_callable=execute_script, dag=dag, )","title":"C\u00f3digo Completo"},{"location":"dags/busca_dados_dag/","text":"busca_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de busca de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script busca_dados.py . Principais Componentes execute_script() Executa o script busca_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a busca de dados. default_args Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha. dag Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG. busca_dados_task Tarefa que executa o script de busca de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence. C\u00f3digo Completo ```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime import subprocess \"\"\" busca_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de busca de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'busca_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - busca_dados_task: Tarefa que executa o script de busca de dados. \"\"\" def execute_script(): \"\"\" Executa o script 'busca_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a busca de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/busca_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'busca_dados_dag', default_args=default_args, schedule_interval='@daily', ) busca_dados_task = PythonOperator( task_id='busca_dados_task', python_callable=execute_script, dag=dag, )","title":"Busca de Dados DAG"},{"location":"dags/busca_dados_dag/#busca_dados_dagpy","text":"Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de busca de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script busca_dados.py .","title":"busca_dados_dag.py"},{"location":"dags/busca_dados_dag/#principais-componentes","text":"","title":"Principais Componentes"},{"location":"dags/busca_dados_dag/#execute_script","text":"Executa o script busca_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a busca de dados.","title":"execute_script()"},{"location":"dags/busca_dados_dag/#default_args","text":"Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha.","title":"default_args"},{"location":"dags/busca_dados_dag/#dag","text":"Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG.","title":"dag"},{"location":"dags/busca_dados_dag/#busca_dados_task","text":"Tarefa que executa o script de busca de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence.","title":"busca_dados_task"},{"location":"dags/busca_dados_dag/#codigo-completo","text":"```python from airflow import DAG from airflow.operators.python_operator import PythonOperator from datetime import datetime import subprocess \"\"\" busca_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de busca de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'busca_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - busca_dados_task: Tarefa que executa o script de busca de dados. \"\"\" def execute_script(): \"\"\" Executa o script 'busca_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a busca de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/busca_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'busca_dados_dag', default_args=default_args, schedule_interval='@daily', ) busca_dados_task = PythonOperator( task_id='busca_dados_task', python_callable=execute_script, dag=dag, )","title":"C\u00f3digo Completo"},{"location":"dags/transformacao_dados_dag/","text":"transformacao_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de transforma\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script transformacao_dados.py . Principais Componentes execute_script() Executa o script transformacao_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a transforma\u00e7\u00e3o de dados. default_args Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha. dag Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG. transformacao_dados_task Tarefa que executa o script de transforma\u00e7\u00e3o de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence. C\u00f3digo Completo ```python from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime import subprocess \"\"\" transformacao_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de transforma\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'transformacao_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - transformacao_dados_task: Tarefa que executa o script de transforma\u00e7\u00e3o de dados. \"\"\" def execute_script(): \"\"\" Executa o script 'transformacao_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a transforma\u00e7\u00e3o de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/transformacao_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'transformacao_dados_dag', default_args=default_args, schedule_interval='@daily', ) transformacao_dados_task = PythonOperator( task_id='transformacao_dados_task', python_callable=execute_script, dag=dag, )","title":"Transforma\u00e7\u00e3o de Dados DAG"},{"location":"dags/transformacao_dados_dag/#transformacao_dados_dagpy","text":"Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de transforma\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script transformacao_dados.py .","title":"transformacao_dados_dag.py"},{"location":"dags/transformacao_dados_dag/#principais-componentes","text":"","title":"Principais Componentes"},{"location":"dags/transformacao_dados_dag/#execute_script","text":"Executa o script transformacao_dados.py usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a transforma\u00e7\u00e3o de dados.","title":"execute_script()"},{"location":"dags/transformacao_dados_dag/#default_args","text":"Argumentos padr\u00e3o para a DAG. owner (str): Dono da DAG. depends_on_past (bool): Se a DAG depende da execu\u00e7\u00e3o anterior. start_date (datetime): Data de in\u00edcio da DAG. retries (int): N\u00famero de tentativas de reexecu\u00e7\u00e3o em caso de falha.","title":"default_args"},{"location":"dags/transformacao_dados_dag/#dag","text":"Defini\u00e7\u00e3o da DAG. dag_id (str): Identificador da DAG. default_args (dict): Argumentos padr\u00e3o para a DAG. schedule_interval (str): Intervalo de agendamento da DAG.","title":"dag"},{"location":"dags/transformacao_dados_dag/#transformacao_dados_task","text":"Tarefa que executa o script de transforma\u00e7\u00e3o de dados. task_id (str): Identificador da tarefa. python_callable (function): Fun\u00e7\u00e3o a ser chamada pela tarefa. dag (DAG): DAG \u00e0 qual a tarefa pertence.","title":"transformacao_dados_task"},{"location":"dags/transformacao_dados_dag/#codigo-completo","text":"```python from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime import subprocess \"\"\" transformacao_dados_dag.py Este m\u00f3dulo define uma DAG (Directed Acyclic Graph) do Airflow para executar o script de transforma\u00e7\u00e3o de dados. A DAG \u00e9 configurada para ser executada diariamente e chama o script 'transformacao_dados.py'. Principais componentes: - execute_script: Fun\u00e7\u00e3o que executa o script Python externo. - default_args: Argumentos padr\u00e3o para a DAG. - dag: Defini\u00e7\u00e3o da DAG. - transformacao_dados_task: Tarefa que executa o script de transforma\u00e7\u00e3o de dados. \"\"\" def execute_script(): \"\"\" Executa o script 'transformacao_dados.py' usando subprocess. Esta fun\u00e7\u00e3o \u00e9 chamada pela tarefa do Airflow para realizar a transforma\u00e7\u00e3o de dados. \"\"\" subprocess.run([\"python\", \"/opt/airflow/scripts/transformacao_dados.py\"], check=True) default_args = { 'owner': 'airflow', 'depends_on_past': False, 'start_date': datetime(2023, 8, 1), 'retries': 1, } dag = DAG( 'transformacao_dados_dag', default_args=default_args, schedule_interval='@daily', ) transformacao_dados_task = PythonOperator( task_id='transformacao_dados_task', python_callable=execute_script, dag=dag, )","title":"C\u00f3digo Completo"},{"location":"scripts/agregando_dados/","text":"agregando_dados.py Este m\u00f3dulo \u00e9 respons\u00e1vel por agregar dados de cervejarias armazenados em arquivos Parquet no S3. Ele l\u00ea os arquivos, realiza a agrega\u00e7\u00e3o por estado e tipo de cervejaria, e salva o resultado em um novo arquivo Parquet no bucket curado do S3. Fun\u00e7\u00f5es Principais listar_arquivos_parquet(bucket_name, prefix=\"\") Lista todos os arquivos Parquet em um bucket S3 espec\u00edfico. Argumentos: - bucket_name (str): Nome do bucket S3. - prefix (str, opcional): Prefixo para filtrar os arquivos. Padr\u00e3o \u00e9 \"\". Retorna: - Lista de chaves (nomes) dos arquivos Parquet encontrados. agregacao_dados(bucket_name, prefix) Agrega dados de cervejarias de m\u00faltiplos arquivos Parquet no S3. Opera\u00e7\u00f5es: 1. Lista arquivos Parquet no bucket S3 especificado. 2. Baixa e l\u00ea cada arquivo Parquet. 3. Agrega os dados por estado e tipo de cervejaria. 4. Combina os resultados de todos os arquivos. 5. Salva o resultado agregado em um novo arquivo Parquet no bucket curado. Argumentos: - bucket_name (str): Nome do bucket S3 contendo os arquivos de origem. - prefix (str): Prefixo para filtrar os arquivos no bucket. Depend\u00eancias pandas pyarrow boto3 dotenv Configura\u00e7\u00e3o O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets. Logging O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de agrega\u00e7\u00e3o. Execu\u00e7\u00e3o Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o agregacao_dados com o bucket raw e sem prefixo.","title":"Agrega\u00e7\u00e3o de Dados"},{"location":"scripts/agregando_dados/#agregando_dadospy","text":"Este m\u00f3dulo \u00e9 respons\u00e1vel por agregar dados de cervejarias armazenados em arquivos Parquet no S3. Ele l\u00ea os arquivos, realiza a agrega\u00e7\u00e3o por estado e tipo de cervejaria, e salva o resultado em um novo arquivo Parquet no bucket curado do S3.","title":"agregando_dados.py"},{"location":"scripts/agregando_dados/#funcoes-principais","text":"","title":"Fun\u00e7\u00f5es Principais"},{"location":"scripts/agregando_dados/#listar_arquivos_parquetbucket_name-prefix","text":"Lista todos os arquivos Parquet em um bucket S3 espec\u00edfico. Argumentos: - bucket_name (str): Nome do bucket S3. - prefix (str, opcional): Prefixo para filtrar os arquivos. Padr\u00e3o \u00e9 \"\". Retorna: - Lista de chaves (nomes) dos arquivos Parquet encontrados.","title":"listar_arquivos_parquet(bucket_name, prefix=\"\")"},{"location":"scripts/agregando_dados/#agregacao_dadosbucket_name-prefix","text":"Agrega dados de cervejarias de m\u00faltiplos arquivos Parquet no S3. Opera\u00e7\u00f5es: 1. Lista arquivos Parquet no bucket S3 especificado. 2. Baixa e l\u00ea cada arquivo Parquet. 3. Agrega os dados por estado e tipo de cervejaria. 4. Combina os resultados de todos os arquivos. 5. Salva o resultado agregado em um novo arquivo Parquet no bucket curado. Argumentos: - bucket_name (str): Nome do bucket S3 contendo os arquivos de origem. - prefix (str): Prefixo para filtrar os arquivos no bucket.","title":"agregacao_dados(bucket_name, prefix)"},{"location":"scripts/agregando_dados/#dependencias","text":"pandas pyarrow boto3 dotenv","title":"Depend\u00eancias"},{"location":"scripts/agregando_dados/#configuracao","text":"O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets.","title":"Configura\u00e7\u00e3o"},{"location":"scripts/agregando_dados/#logging","text":"O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de agrega\u00e7\u00e3o.","title":"Logging"},{"location":"scripts/agregando_dados/#execucao","text":"Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o agregacao_dados com o bucket raw e sem prefixo.","title":"Execu\u00e7\u00e3o"},{"location":"scripts/busca_dados/","text":"busca_dados.py Este m\u00f3dulo \u00e9 respons\u00e1vel por extrair dados de cervejarias da API OpenBreweryDB, transform\u00e1-los em um DataFrame pandas, e salvar os resultados como um arquivo CSV no S3. Fun\u00e7\u00f5es Principais extracao_api(api_url: str) -> Dict[str, Any] Realiza uma requisi\u00e7\u00e3o GET para a API especificada e retorna os dados JSON. Argumentos: - api_url (str): URL da API para fazer a requisi\u00e7\u00e3o. Retorna: - Dict[str, Any] : Dados JSON retornados pela API. Erros: - requests.exceptions.RequestException : Se ocorrer um erro na requisi\u00e7\u00e3o \u00e0 API. cria_dataframe(lista: list) -> DataFrame Converte uma lista de dicion\u00e1rios em um DataFrame pandas. Argumentos: - lista (list): Lista de dicion\u00e1rios contendo os dados das cervejarias. Retorna: - DataFrame : DataFrame pandas criado a partir da lista de dados. Erros: - Exception : Se ocorrer um erro durante a convers\u00e3o para DataFrame. salvando_s3(df: pd.DataFrame, bucket: str, key: str) Converte um DataFrame para CSV e o salva em um bucket S3. Argumentos: - df (pd.DataFrame): DataFrame a ser salvo. - bucket (str): Nome do bucket S3 onde o arquivo ser\u00e1 salvo. - key (str): Chave (nome do arquivo) para o arquivo no S3. Erros: - Exception : Se ocorrer um erro durante a convers\u00e3o para CSV ou upload para S3. Depend\u00eancias requests pandas boto3 dotenv StringIO (from io) DataFrame (from pandas) Dict, Any (from typing) datetime (from datetime) logging Configura\u00e7\u00e3o O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets. Logging O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de extra\u00e7\u00e3o. Execu\u00e7\u00e3o Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o extracao_api com a URL da API OpenBreweryDB, converte os dados em um DataFrame, e salva o resultado em um arquivo CSV no bucket S3 configurado.","title":"Busca de Dados"},{"location":"scripts/busca_dados/#busca_dadospy","text":"Este m\u00f3dulo \u00e9 respons\u00e1vel por extrair dados de cervejarias da API OpenBreweryDB, transform\u00e1-los em um DataFrame pandas, e salvar os resultados como um arquivo CSV no S3.","title":"busca_dados.py"},{"location":"scripts/busca_dados/#funcoes-principais","text":"","title":"Fun\u00e7\u00f5es Principais"},{"location":"scripts/busca_dados/#extracao_apiapi_url-str-dictstr-any","text":"Realiza uma requisi\u00e7\u00e3o GET para a API especificada e retorna os dados JSON. Argumentos: - api_url (str): URL da API para fazer a requisi\u00e7\u00e3o. Retorna: - Dict[str, Any] : Dados JSON retornados pela API. Erros: - requests.exceptions.RequestException : Se ocorrer um erro na requisi\u00e7\u00e3o \u00e0 API.","title":"extracao_api(api_url: str) -&gt; Dict[str, Any]"},{"location":"scripts/busca_dados/#cria_dataframelista-list-dataframe","text":"Converte uma lista de dicion\u00e1rios em um DataFrame pandas. Argumentos: - lista (list): Lista de dicion\u00e1rios contendo os dados das cervejarias. Retorna: - DataFrame : DataFrame pandas criado a partir da lista de dados. Erros: - Exception : Se ocorrer um erro durante a convers\u00e3o para DataFrame.","title":"cria_dataframe(lista: list) -&gt; DataFrame"},{"location":"scripts/busca_dados/#salvando_s3df-pddataframe-bucket-str-key-str","text":"Converte um DataFrame para CSV e o salva em um bucket S3. Argumentos: - df (pd.DataFrame): DataFrame a ser salvo. - bucket (str): Nome do bucket S3 onde o arquivo ser\u00e1 salvo. - key (str): Chave (nome do arquivo) para o arquivo no S3. Erros: - Exception : Se ocorrer um erro durante a convers\u00e3o para CSV ou upload para S3.","title":"salvando_s3(df: pd.DataFrame, bucket: str, key: str)"},{"location":"scripts/busca_dados/#dependencias","text":"requests pandas boto3 dotenv StringIO (from io) DataFrame (from pandas) Dict, Any (from typing) datetime (from datetime) logging","title":"Depend\u00eancias"},{"location":"scripts/busca_dados/#configuracao","text":"O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets.","title":"Configura\u00e7\u00e3o"},{"location":"scripts/busca_dados/#logging","text":"O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de extra\u00e7\u00e3o.","title":"Logging"},{"location":"scripts/busca_dados/#execucao","text":"Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o extracao_api com a URL da API OpenBreweryDB, converte os dados em um DataFrame, e salva o resultado em um arquivo CSV no bucket S3 configurado.","title":"Execu\u00e7\u00e3o"},{"location":"scripts/transformacao_dados/","text":"transformacao_dados.py Este m\u00f3dulo \u00e9 respons\u00e1vel por transformar dados de cervejarias armazenados em arquivos CSV no S3. Ele l\u00ea o arquivo CSV mais recente, realiza transforma\u00e7\u00f5es nos dados, e salva os resultados como arquivos Parquet separados por estado no bucket raw do S3. Fun\u00e7\u00f5es Principais listar_arquivos_csv(bucket_name: str, prefix: str = \"\") -> List[str] Lista todos os arquivos CSV em um bucket S3 espec\u00edfico. Argumentos: - bucket_name (str): Nome do bucket S3. - prefix (str, opcional): Prefixo para filtrar os arquivos. Padr\u00e3o \u00e9 \"\". Retorna: - List[str] : Lista de chaves (nomes) dos arquivos CSV encontrados. Erros: - FileNotFoundError : Se nenhum arquivo CSV for encontrado no bucket. baixar_arquivo_csv(bucket_name: str, arquivo: str) -> str Baixa um arquivo CSV espec\u00edfico de um bucket S3. Argumentos: - bucket_name (str): Nome do bucket S3. - arquivo (str): Chave (nome) do arquivo a ser baixado. Retorna: - str : Caminho local do arquivo baixado. transformar_dados(caminho_csv: str) -> pd.DataFrame L\u00ea um arquivo CSV e realiza transforma\u00e7\u00f5es nos dados. Transforma\u00e7\u00f5es realizadas: - Normaliza os nomes dos estados (strip e title case). - Remove linhas com valores nulos na coluna 'state'. Argumentos: - caminho_csv (str): Caminho local do arquivo CSV. Retorna: - pd.DataFrame : DataFrame com os dados transformados. salvar_dados_parquet(df: pd.DataFrame, state: str, bucket_name: str) Salva um DataFrame como arquivo Parquet no S3, separado por estado. Argumentos: - df (pd.DataFrame): DataFrame a ser salvo. - state (str): Nome do estado para o qual os dados est\u00e3o sendo salvos. - bucket_name (str): Nome do bucket S3 onde o arquivo ser\u00e1 salvo. transformacao_dados(entrada_bucket: str, saida_bucket: str, prefix: str) Fun\u00e7\u00e3o principal que orquestra o processo de transforma\u00e7\u00e3o dos dados. Esta fun\u00e7\u00e3o realiza as seguintes opera\u00e7\u00f5es: 1. Lista e seleciona o arquivo CSV mais recente no bucket de entrada. 2. Baixa o arquivo CSV selecionado. 3. Transforma os dados. 4. Agrupa os dados por estado. 5. Salva os dados transformados como arquivos Parquet separados por estado no bucket de sa\u00edda. Argumentos: - entrada_bucket (str): Nome do bucket S3 contendo os arquivos CSV de origem. - saida_bucket (str): Nome do bucket S3 onde os arquivos Parquet ser\u00e3o salvos. - prefix (str): Prefixo para filtrar os arquivos CSV no bucket de entrada. Depend\u00eancias pandas pyarrow boto3 dotenv os datetime logging Configura\u00e7\u00e3o O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets. Logging O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de transforma\u00e7\u00e3o. Execu\u00e7\u00e3o Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o transformacao_dados com os buckets de entrada e sa\u00edda configurados, e o prefixo para filtrar os arquivos CSV no bucket de entrada.","title":"Transforma\u00e7\u00e3o de Dados"},{"location":"scripts/transformacao_dados/#transformacao_dadospy","text":"Este m\u00f3dulo \u00e9 respons\u00e1vel por transformar dados de cervejarias armazenados em arquivos CSV no S3. Ele l\u00ea o arquivo CSV mais recente, realiza transforma\u00e7\u00f5es nos dados, e salva os resultados como arquivos Parquet separados por estado no bucket raw do S3.","title":"transformacao_dados.py"},{"location":"scripts/transformacao_dados/#funcoes-principais","text":"","title":"Fun\u00e7\u00f5es Principais"},{"location":"scripts/transformacao_dados/#listar_arquivos_csvbucket_name-str-prefix-str-liststr","text":"Lista todos os arquivos CSV em um bucket S3 espec\u00edfico. Argumentos: - bucket_name (str): Nome do bucket S3. - prefix (str, opcional): Prefixo para filtrar os arquivos. Padr\u00e3o \u00e9 \"\". Retorna: - List[str] : Lista de chaves (nomes) dos arquivos CSV encontrados. Erros: - FileNotFoundError : Se nenhum arquivo CSV for encontrado no bucket.","title":"listar_arquivos_csv(bucket_name: str, prefix: str = \"\") -&gt; List[str]"},{"location":"scripts/transformacao_dados/#baixar_arquivo_csvbucket_name-str-arquivo-str-str","text":"Baixa um arquivo CSV espec\u00edfico de um bucket S3. Argumentos: - bucket_name (str): Nome do bucket S3. - arquivo (str): Chave (nome) do arquivo a ser baixado. Retorna: - str : Caminho local do arquivo baixado.","title":"baixar_arquivo_csv(bucket_name: str, arquivo: str) -&gt; str"},{"location":"scripts/transformacao_dados/#transformar_dadoscaminho_csv-str-pddataframe","text":"L\u00ea um arquivo CSV e realiza transforma\u00e7\u00f5es nos dados. Transforma\u00e7\u00f5es realizadas: - Normaliza os nomes dos estados (strip e title case). - Remove linhas com valores nulos na coluna 'state'. Argumentos: - caminho_csv (str): Caminho local do arquivo CSV. Retorna: - pd.DataFrame : DataFrame com os dados transformados.","title":"transformar_dados(caminho_csv: str) -&gt; pd.DataFrame"},{"location":"scripts/transformacao_dados/#salvar_dados_parquetdf-pddataframe-state-str-bucket_name-str","text":"Salva um DataFrame como arquivo Parquet no S3, separado por estado. Argumentos: - df (pd.DataFrame): DataFrame a ser salvo. - state (str): Nome do estado para o qual os dados est\u00e3o sendo salvos. - bucket_name (str): Nome do bucket S3 onde o arquivo ser\u00e1 salvo.","title":"salvar_dados_parquet(df: pd.DataFrame, state: str, bucket_name: str)"},{"location":"scripts/transformacao_dados/#transformacao_dadosentrada_bucket-str-saida_bucket-str-prefix-str","text":"Fun\u00e7\u00e3o principal que orquestra o processo de transforma\u00e7\u00e3o dos dados. Esta fun\u00e7\u00e3o realiza as seguintes opera\u00e7\u00f5es: 1. Lista e seleciona o arquivo CSV mais recente no bucket de entrada. 2. Baixa o arquivo CSV selecionado. 3. Transforma os dados. 4. Agrupa os dados por estado. 5. Salva os dados transformados como arquivos Parquet separados por estado no bucket de sa\u00edda. Argumentos: - entrada_bucket (str): Nome do bucket S3 contendo os arquivos CSV de origem. - saida_bucket (str): Nome do bucket S3 onde os arquivos Parquet ser\u00e3o salvos. - prefix (str): Prefixo para filtrar os arquivos CSV no bucket de entrada.","title":"transformacao_dados(entrada_bucket: str, saida_bucket: str, prefix: str)"},{"location":"scripts/transformacao_dados/#dependencias","text":"pandas pyarrow boto3 dotenv os datetime logging","title":"Depend\u00eancias"},{"location":"scripts/transformacao_dados/#configuracao","text":"O m\u00f3dulo utiliza vari\u00e1veis de ambiente carregadas de um arquivo .env para configura\u00e7\u00f5es sens\u00edveis como chaves de acesso AWS e nomes de buckets.","title":"Configura\u00e7\u00e3o"},{"location":"scripts/transformacao_dados/#logging","text":"O m\u00f3dulo utiliza logging para registrar informa\u00e7\u00f5es e erros durante a execu\u00e7\u00e3o do processo de transforma\u00e7\u00e3o.","title":"Logging"},{"location":"scripts/transformacao_dados/#execucao","text":"Quando executado como script principal, o m\u00f3dulo chama a fun\u00e7\u00e3o transformacao_dados com os buckets de entrada e sa\u00edda configurados, e o prefixo para filtrar os arquivos CSV no bucket de entrada.","title":"Execu\u00e7\u00e3o"}]}